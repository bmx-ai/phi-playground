{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "This notebook evaluates phi-2 on @openati GSM8K dataset for mathematical reasoning\n",
    "\n",
    "GSM8K consists of 8.5K high quality grade school math problems created by human problem writers.\n",
    "\n",
    "GSM8K's main difficulty lies in both properly interpreting a question and reasoning through the steps to solve it.\n",
    "\n",
    "Sampling strategies:\n",
    "\n",
    "1. At test time, we judge performance by autoregressively sampling a single _low temperature_ solution and checking whether the final answer is correct.\n",
    "1.\n",
    "\n",
    "We use a low temperature (T = 0) to generate test@1 samples and we use a higher temperature (T = 0.7) to generate test@100 samples.\n",
    "\n",
    "- [Paper](https://arxiv.org/abs/2110.14168)\n",
    "- [OpenAI blog post](https://openai.com/research/solving-math-word-problems)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/openai/grade-school-math --depth 1 ./grade-school-math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install ipywidgets -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -e ./grade-school-math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup notebook\n",
    "import os\n",
    "import sys\n",
    "\n",
    "local_path = !pwd\n",
    "workspace_dir = os.path.abspath(os.path.join(local_path[0], \"..\", \"..\"))\n",
    "print('workspace:', workspace_dir)\n",
    "sys.path.append(os.path.join(workspace_dir, \"models\"))\n",
    "model_path = os.path.join(workspace_dir, '.cache', 'models', 'microsoft', 'phi-2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlx.core as mx\n",
    "\n",
    "mx.set_default_device(mx.gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import microsoft_phi2_model as phi\n",
    "\n",
    "model, tokenizer = phi.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlx.core as mx\n",
    "import textwrap\n",
    "import re\n",
    "\n",
    "ANS_RE = re.compile(r\"#### (\\-?[0-9\\.\\,]+)\")\n",
    "INVALID_ANS = \"[invalid]\"\n",
    "\n",
    "\n",
    "def extract_answer(completion):\n",
    "    match = ANS_RE.search(completion)\n",
    "    if match:\n",
    "        match_str = match.group(1).strip()\n",
    "        match_str = match_str.replace(\",\", \"\")\n",
    "        return match_str\n",
    "    else:\n",
    "        return INVALID_ANS\n",
    "\n",
    "\n",
    "def is_correct(model_completion, gt_example):\n",
    "    gt_answer = extract_answer(gt_example[\"answer\"])\n",
    "    assert gt_answer != INVALID_ANS\n",
    "    return extract_answer(model_completion) == gt_answer\n",
    "\n",
    "\n",
    "def evaluate(examples, verbose=False):\n",
    "    temp = 0.0\n",
    "    max_tokens = 512\n",
    "    tokens = []\n",
    "    skip = 0\n",
    "    REPLACEMENT_CHAR = \"\\ufffd\"\n",
    "\n",
    "    prompt_tokens = [\n",
    "        tokenizer.encode(\"Question: \" + ex[\"question\"] + \"\\nAnswer:\") for ex in examples\n",
    "    ][0]\n",
    "\n",
    "    tokens = prompt_tokens[:]\n",
    "\n",
    "    for (token, prob), n in zip(\n",
    "        phi.generate_step(mx.array(prompt_tokens), model, temp), range(max_tokens)\n",
    "    ):\n",
    "        if token == tokenizer.eos_token_id:\n",
    "            break\n",
    "        tokens.append(token.item())\n",
    "\n",
    "        # s = \"\\n\".join(textwrap.wrap(s, width=120))\n",
    "        if verbose:\n",
    "            ss = tokenizer.decode(tokens)\n",
    "            if REPLACEMENT_CHAR not in ss:\n",
    "                sys.stdout.write(ss[skip:])\n",
    "                sys.stdout.flush()\n",
    "                skip = len(ss)\n",
    "    output = tokenizer.decode(tokens)\n",
    "    line = [l for l in output.split(\"\\n\") if l.strip()][-1]\n",
    "    candidates = re.findall(\"\\d+\", line)\n",
    "    if not candidates:\n",
    "        return False, output, line\n",
    "    answer_gold = extract_answer(ex)\n",
    "    correct = is_correct(output, answer_gold)\n",
    "    return (\n",
    "        correct,\n",
    "        candidates[-1],\n",
    "        answer_gold,\n",
    "        output,\n",
    "        line,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import json\n",
    "\n",
    "\n",
    "def read_jsonl(path: str):\n",
    "    with open(path) as fh:\n",
    "        return [json.loads(line) for line in fh.readlines() if line]\n",
    "\n",
    "\n",
    "def get_examples(split):\n",
    "    path = os.path.join(\n",
    "        \"./grade-school-math\", \"grade_school_math\", \"data\", f\"{split}.jsonl\"\n",
    "    )\n",
    "    examples = read_jsonl(path)\n",
    "\n",
    "    for ex in examples:\n",
    "        ex.update(question=ex[\"question\"] + \"\\n\")\n",
    "        ex.update(answer=ex[\"answer\"] + \"<|endoftext|>\")\n",
    "\n",
    "    return examples\n",
    "\n",
    "\n",
    "split = \"train\"\n",
    "train_examples = get_examples(split)\n",
    "print(f\"{len(train_examples)} {split} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex = random.choice(train_examples)\n",
    "evaluate([ex], verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate on the Test Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "split = \"test\"\n",
    "test_examples = get_examples(split)\n",
    "# ds = dataset.GSMDataset(tokenizer=tokenizer, examples=train_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "correct = []\n",
    "pbar = tqdm(total=len(test_examples))\n",
    "for ex in test_examples:\n",
    "    ok, a, b, *_ = evaluate(ex)\n",
    "    correct.append(ok)\n",
    "    pbar.update(1)\n",
    "    pbar.set_description(f\"accuracy: {np.sum(correct) / len(correct)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorchl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
